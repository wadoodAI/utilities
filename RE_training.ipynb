{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94028e6-a048-4592-ad43-2c9e9d71070d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting johnsnowlabs\n",
      "  Obtaining dependency information for johnsnowlabs from https://files.pythonhosted.org/packages/07/d9/9a12fd966c608c76eb7ff2e1ba5d2746d98626164038df8ef4564cf85ca8/johnsnowlabs-5.0.1-py3-none-any.whl.metadata\n",
      "  Downloading johnsnowlabs-5.0.1-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pyspark==3.1.2 in /usr/local/lib/python3.8/dist-packages (from johnsnowlabs) (3.1.2)\n",
      "Collecting spark-nlp==5.0.1 (from johnsnowlabs)\n",
      "  Obtaining dependency information for spark-nlp==5.0.1 from https://files.pythonhosted.org/packages/d4/5e/ff63a474a678d713f8de5f874105fa3329bd0db5bf6110dd998c3fd927ec/spark_nlp-5.0.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading spark_nlp-5.0.1-py2.py3-none-any.whl.metadata (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nlu==4.2.2 (from johnsnowlabs)\n",
      "  Obtaining dependency information for nlu==4.2.2 from https://files.pythonhosted.org/packages/c4/7d/6e249988e9f78ba118c4f89de492508ea98b5330526267762492aadf1654/nlu-4.2.2-py3-none-any.whl.metadata\n",
      "  Downloading nlu-4.2.2-py3-none-any.whl.metadata (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spark-nlp-display==4.1 (from johnsnowlabs)\n",
      "  Downloading spark_nlp_display-4.1-py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.4/95.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from johnsnowlabs) (1.24.4)\n",
      "Collecting dataclasses (from johnsnowlabs)\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from johnsnowlabs) (2.31.0)\n",
      "Collecting databricks-api (from johnsnowlabs)\n",
      "  Obtaining dependency information for databricks-api from https://files.pythonhosted.org/packages/d0/cc/6c3f9cd8b2b6c7a45c95b94d334bc51f1579d875bbfac0ecb8accdb2f756/databricks_api-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading databricks_api-0.9.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pydantic==1.10.11 (from johnsnowlabs)\n",
      "  Obtaining dependency information for pydantic==1.10.11 from https://files.pythonhosted.org/packages/4e/d7/04c964cda3b5e2c05d40d89fffa4449512c757476c9dd6aee9154a2e2603/pydantic-1.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic-1.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting colorama (from johnsnowlabs)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting pyarrow>=0.16.0 (from nlu==4.2.2->johnsnowlabs)\n",
      "  Obtaining dependency information for pyarrow>=0.16.0 from https://files.pythonhosted.org/packages/e1/91/676b6ef5181fd0229ec35477eb94ff55fc5114ebab7a4669db311ddc9385/pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pandas>=1.3.5 (from nlu==4.2.2->johnsnowlabs)\n",
      "  Obtaining dependency information for pandas>=1.3.5 from https://files.pythonhosted.org/packages/f8/7f/5b047effafbdd34e52c9e2d7e44f729a0655efafb22198c45cf692cdc157/pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic==1.10.11->johnsnowlabs) (4.7.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.8/dist-packages (from pyspark==3.1.2->johnsnowlabs) (0.10.9)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from spark-nlp-display==4.1->johnsnowlabs) (8.12.2)\n",
      "Requirement already satisfied: svgwrite==1.4 in /usr/local/lib/python3.8/dist-packages (from spark-nlp-display==4.1->johnsnowlabs) (1.4)\n",
      "Collecting databricks-cli (from databricks-api->johnsnowlabs)\n",
      "  Downloading databricks-cli-0.17.7.tar.gz (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->johnsnowlabs) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->johnsnowlabs) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->johnsnowlabs) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->johnsnowlabs) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.5->nlu==4.2.2->johnsnowlabs) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.5->nlu==4.2.2->johnsnowlabs) (2023.3)\n",
      "Collecting tzdata>=2022.1 (from pandas>=1.3.5->nlu==4.2.2->johnsnowlabs)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click>=7.0 (from databricks-cli->databricks-api->johnsnowlabs)\n",
      "  Obtaining dependency information for click>=7.0 from https://files.pythonhosted.org/packages/1a/70/e63223f8116931d365993d4a6b7ef653a4d920b41d03de7c59499962821f/click-8.1.6-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli->databricks-api->johnsnowlabs) (3.2.2)\n",
      "Collecting pyjwt>=1.7.0 (from databricks-cli->databricks-api->johnsnowlabs)\n",
      "  Obtaining dependency information for pyjwt>=1.7.0 from https://files.pythonhosted.org/packages/2b/4f/e04a8067c7c96c364cef7ef73906504e2f40d690811c021e1a1901473a19/PyJWT-2.8.0-py3-none-any.whl.metadata\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli->databricks-api->johnsnowlabs) (1.16.0)\n",
      "Collecting tabulate>=0.7.7 (from databricks-cli->databricks-api->johnsnowlabs)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (5.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython->spark-nlp-display==4.1->johnsnowlabs) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython->spark-nlp-display==4.1->johnsnowlabs) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->spark-nlp-display==4.1->johnsnowlabs) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->spark-nlp-display==4.1->johnsnowlabs) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->spark-nlp-display==4.1->johnsnowlabs) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython->spark-nlp-display==4.1->johnsnowlabs) (0.2.2)\n",
      "Downloading johnsnowlabs-5.0.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nlu-4.2.2-py3-none-any.whl (641 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m641.3/641.3 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spark_nlp-5.0.1-py2.py3-none-any.whl (499 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.0/499.0 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading databricks_api-0.9.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.17.7-py3-none-any.whl size=143877 sha256=bfc75a39182a45c776cd494e4aafc1b6e19ef61f65b358c58ae2641ce09b04fa\n",
      "  Stored in directory: /root/.cache/pip/wheels/65/c6/f0/85c95278b97ff8245c981b81f00bc05eb289cf79f884db2cbc\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: spark-nlp, dataclasses, tzdata, tabulate, pyjwt, pydantic, pyarrow, colorama, click, pandas, databricks-cli, nlu, databricks-api, spark-nlp-display, johnsnowlabs\n",
      "  Attempting uninstall: spark-nlp\n",
      "    Found existing installation: spark-nlp 5.0.0\n",
      "    Uninstalling spark-nlp-5.0.0:\n",
      "      Successfully uninstalled spark-nlp-5.0.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.5\n",
      "    Uninstalling pandas-1.1.5:\n",
      "      Successfully uninstalled pandas-1.1.5\n",
      "  Attempting uninstall: spark-nlp-display\n",
      "    Found existing installation: spark-nlp-display 4.4\n",
      "    Uninstalling spark-nlp-display-4.4:\n",
      "      Successfully uninstalled spark-nlp-display-4.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spark-ocr 4.4.3 requires craft-text-detector==0.4.2, which is not installed.\n",
      "spark-ocr 4.4.3 requires implicits==1.0.2, which is not installed.\n",
      "spark-ocr 4.4.3 requires pillow==9.0.1, which is not installed.\n",
      "spark-ocr 4.4.3 requires scikit-image==0.18.1, which is not installed.\n",
      "spark-ocr 4.4.3 requires torchvision<=0.13.0,>=0.7.0, which is not installed.\n",
      "spark-nlp-jsl 5.0.0 requires spark-nlp==5.0.0, but you have spark-nlp 5.0.1 which is incompatible.\n",
      "spark-ocr 4.4.3 requires numpy<=1.24.3,>=1.21.6, but you have numpy 1.24.4 which is incompatible.\n",
      "spark-ocr 4.4.3 requires pyspark==3.2.1, but you have pyspark 3.1.2 which is incompatible.\n",
      "spark-ocr 4.4.3 requires spark-nlp==4.4.4, but you have spark-nlp 5.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed click-8.1.6 colorama-0.4.6 databricks-api-0.9.0 databricks-cli-0.17.7 dataclasses-0.6 johnsnowlabs-5.0.1 nlu-4.2.2 pandas-2.0.3 pyarrow-12.0.1 pydantic-1.10.11 pyjwt-2.8.0 spark-nlp-5.0.1 spark-nlp-display-4.1 tabulate-0.9.0 tzdata-2023.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install johnsnowlabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9182a1-2bd3-4af2-af86-918ddccb245c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m🚨 Visual NLP installation seems broken\u001b[39m, there was an exception while importing it. It will not be available on the nlp.xx module\n",
      "You can run \u001b[92m nlp.install(refresh_install=True, force_browser=True) \u001b[39m to re-install latest version. \n",
      "Error Message : No module named 'implicits'\n",
      "Error Trace: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/johnsnowlabs/visual.py\", line 18, in <module>\n",
      "    from sparkocr.databricks import isRunningInDatabricks\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/sparkocr/databricks.py\", line 4, in <module>\n",
      "    from implicits import implicits\n",
      "ModuleNotFoundError: No module named 'implicits'\n",
      "\n",
      "📋 Loading license number 0 from /root/.johnsnowlabs/licenses/license_number_0_for_Spark-Healthcare_Spark-OCR.json\n",
      "👌 JSL-Home is up to date! \n",
      "👌 Everything is already installed, no changes made\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/10 10:37:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/10 10:37:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👌 Launched \u001b[92mcpu optimized\u001b[39m session with with: 🚀Spark-NLP==5.0.1, 💊Spark-Healthcare==5.0.1, running on ⚡ PySpark==3.1.2\n"
     ]
    }
   ],
   "source": [
    "from johnsnowlabs import nlp, medical, visual\n",
    "\n",
    "# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n",
    "nlp.install()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Automatically load license data and start a session with all jars user has access to\n",
    "\n",
    "spark = nlp.start()\n",
    "\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql as SQL\n",
    "from pyspark import keyword_only\n",
    "\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4b08a3-4e40-4856-a07a-7b9d9d88ad47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e21fd29e-27fc-46c0-9f07-06b63ff4eb88",
   "metadata": {},
   "source": [
    "# RE Discussion\n",
    "\n",
    "1. Basic training pipeline(training + evaluation) - from 03.4.Resume_RelationExtractionApproach_Training.ipynb\n",
    "2. Training pipeline(training+ evaluation) with redl models\n",
    "\n",
    "\n",
    "Questions:\n",
    "1. During re model training, a NN architecture is specified. Is this common across all non redl models?\n",
    "2. The class used while training non dl model is not compatible with redl models(see error at  the bottom). What class can be used for redl models?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da051d-85fb-4189-9d8e-3bec4890619f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be6bc007-c053-4bcc-ac6b-71cbcfc7c008",
   "metadata": {},
   "source": [
    "## Basic training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37074a-dd1c-49cb-bbe2-3e767a526e82",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dataset preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6150618-41ab-4c26-af70-b2b2451dde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/i2b2_clinical_rel_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274726d9-2cb8-4e8b-b04a-e842a76f5850",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.option(\"header\",\"true\").format(\"csv\").load(\"i2b2_clinical_rel_dataset.csv\")\n",
    "\n",
    "data = data.select( 'sentence','firstCharEnt1','firstCharEnt2','lastCharEnt1','lastCharEnt2', \"chunk1\", \"chunk2\", \"label1\", \"label2\",'rel','dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1918575d-4848-429d-b3d8-b109a508cd75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_map = {\n",
    "    \"begin1\": \"firstCharEnt1\",\n",
    "    \"end1\": \"lastCharEnt1\",\n",
    "    \"begin2\": \"firstCharEnt2\",\n",
    "    \"end2\": \"lastCharEnt2\",\n",
    "    \"chunk1\": \"chunk1\",\n",
    "    \"chunk2\": \"chunk2\",\n",
    "    \"label1\": \"label1\",\n",
    "    \"label2\": \"label2\"\n",
    "}\n",
    "\n",
    "# apply preprocess function to dataframe\n",
    "data = medical.REDatasetHelper(data).create_annotation_column(\n",
    "    column_map,\n",
    "    ner_column_name=\"train_ner_chunks\" # optional, default train_ner_chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457c6694-41c7-44a0-84eb-4443da0794eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add relation direction\n",
    "\n",
    "@F.udf(T.StringType())\n",
    "def encodeRelationDirection(rel, begin1, begin2):\n",
    "    if rel != \"O\":\n",
    "        if begin1 > begin2:\n",
    "            return \"leftwards\"\n",
    "        else:\n",
    "            return \"rightwards\"\n",
    "    else:\n",
    "        return \"both\"\n",
    "\n",
    "\n",
    "\n",
    "data = data.withColumn(\"rel_dir\", encodeRelationDirection(\"rel\", \"firstCharEnt1\", \"lastCharEnt2\"))\n",
    "\n",
    "train_data = data.where(\"dataset='train'\")\n",
    "test_data = data.where(\"dataset='test'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c6cd3-724e-451c-8464-2d22606ef723",
   "metadata": {},
   "source": [
    "### Loading the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d19bab47-6679-4433-9e4d-b10a0a7b3747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[ | ]embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "pos_clinical download started this may take some time.\n",
      "Approximate size to download 1.5 MB\n",
      "[ | ]pos_clinical download started this may take some time.\n",
      "Approximate size to download 1.5 MB\n",
      "Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "dependency_conllu download started this may take some time.\n",
      "Approximate size to download 16.7 MB\n",
      "[ | ]dependency_conllu download started this may take some time.\n",
      "Approximate size to download 16.7 MB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==============>                                           (1 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==============>                                           (1 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documenter = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"sentence\")\\\n",
    "    .setOutputCol(\"sentences\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "    .setInputCols([\"sentences\"])\\\n",
    "    .setOutputCol(\"tokens\")\\\n",
    "\n",
    "words_embedder = nlp.WordEmbeddingsModel()\\\n",
    "    .pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "pos_tagger = nlp.PerceptronModel()\\\n",
    "    .pretrained(\"pos_clinical\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentences\", \"tokens\"])\\\n",
    "    .setOutputCol(\"pos_tags\")\n",
    "\n",
    "dependency_parser = nlp.DependencyParserModel()\\\n",
    "    .pretrained(\"dependency_conllu\", \"en\")\\\n",
    "    .setInputCols([\"sentences\", \"pos_tags\", \"tokens\"])\\\n",
    "    .setOutputCol(\"dependencies\")\n",
    "\n",
    "finisher = nlp.Finisher()\\\n",
    "    .setInputCols([\"relations\"])\\\n",
    "    .setOutputCols([\"relations_out\"])\\\n",
    "    .setCleanAnnotations(False)\\\n",
    "    .setValueSplitSymbol(\",\")\\\n",
    "    .setAnnotationSplitSymbol(\",\")\\\n",
    "    .setOutputAsArray(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e88e0fc-77be-4fcd-8ba1-2193e46cd28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re_clinical download started this may take some time.\n",
      "[ | ]re_clinical download started this may take some time.\n",
      "Approximate size to download 6 MB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 09:20:37.672017: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TrWP', 'TrNAP', 'TrCP', 'PIP', 'TeCP', 'TeRP', 'TrIP', 'TrAP', 'O']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_re_Model = medical.RelationExtractionModel()\\\n",
    "    .pretrained(\"re_clinical\", \"en\", 'clinical/models')\\\n",
    "    .setInputCols([\"embeddings\", \"pos_tags\", \"train_ner_chunks\", \"dependencies\"])\\\n",
    "    .setOutputCol(\"relations\")\n",
    "\n",
    "clinical_re_Model.getClasses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db108c3-6873-41a9-9152-4c0ba0dbaa06",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c58931-517c-48ba-bfa0-baf479e1f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_pipeline = nlp.Pipeline(stages=[\n",
    "    documenter,\n",
    "    tokenizer,\n",
    "    words_embedder,\n",
    "    pos_tagger,\n",
    "    dependency_parser,\n",
    "    clinical_re_Model,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dccbe62c-9c23-4c66-9e3c-0ac082245648",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = finetune_pipeline.fit(test_data).transform(test_data)\n",
    "result_test_df = result.select('rel', 'relations.result').toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe1a1e75-b8d5-49dd-91af-b6cc4b9f07f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.60      0.50      0.54       274\n",
      "         PIP       0.37      0.22      0.27        88\n",
      "        TeCP       0.00      0.00      0.00        15\n",
      "        TeRP       0.34      0.84      0.48       116\n",
      "        TrAP       0.62      0.24      0.35       108\n",
      "        TrCP       0.20      0.05      0.08        20\n",
      "        TrIP       0.00      0.00      0.00        11\n",
      "       TrNAP       0.40      0.22      0.29         9\n",
      "        TrWP       0.04      0.11      0.06         9\n",
      "\n",
      "    accuracy                           0.44       650\n",
      "   macro avg       0.29      0.24      0.23       650\n",
      "weighted avg       0.48      0.44      0.42       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pd_test_results = result_test_df.explode(\"result\").fillna(\"O\")\n",
    "\n",
    "print(classification_report(pd_test_results[\"rel\"], pd_test_results[\"result\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092c250-af77-444f-a5af-6961f0f20b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a5a92ed-a14e-4e9e-be81-f8d4f932548f",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Questions:\n",
    "1. This was the architecture mentioned for re_clinical. Can the same be used for all other re models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c25f82af-dca8-4258-a103-b52713a24442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sparknlp_jsl.annotator import TFGraphBuilder\n",
    "\n",
    "graph_folder= \"./tf_graphs\"\n",
    "\n",
    "re_graph_builder = medical.TFGraphBuilder()\\\n",
    "    .setModelName(\"relation_extraction\")\\\n",
    "    .setInputCols([\"embeddings\", \"pos_tags\", \"train_ner_chunks\", \"dependencies\"]) \\\n",
    "    .setLabelColumn(\"rel\")\\\n",
    "    .setGraphFolder(graph_folder)\\\n",
    "    .setGraphFile(\"re_graph.pb\")\\\n",
    "    .setHiddenLayers([300, 200])\\\n",
    "    .setHiddenAct(\"relu\")\\\n",
    "    .setHiddenActL2(True)\\\n",
    "    .setHiddenWeightsL2(False)\\\n",
    "    .setBatchNorm(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ae5ad49-f44a-4bb0-b980-2a036b33d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reApproach_finetune = medical.RelationExtractionApproach()\\\n",
    "    .setInputCols([\"embeddings\", \"pos_tags\", \"train_ner_chunks\", \"dependencies\"])\\\n",
    "    .setOutputCol(\"relations\")\\\n",
    "    .setLabelColumn(\"rel\")\\\n",
    "    .setEpochsNumber(50)\\\n",
    "    .setBatchSize(200)\\\n",
    "    .setDropout(0.5)\\\n",
    "    .setLearningRate(0.001)\\\n",
    "    .setFixImbalance(True)\\\n",
    "    .setFromEntity(\"firstCharEnt1\", \"lastCharEnt1\", \"label1\")\\\n",
    "    .setToEntity(\"firstCharEnt2\", \"lastCharEnt2\", \"label2\")\\\n",
    "    .setPretrainedModelPath(\"/root/cache_pretrained/re_clinical_en_2.5.5_2.4_1596928426753\")\\\n",
    "    .setОverrideExistingLabels(False)\n",
    "\n",
    "finetune_pipeline = nlp.Pipeline(stages=[\n",
    "    documenter,\n",
    "    tokenizer,\n",
    "    words_embedder,\n",
    "    pos_tagger,\n",
    "    dependency_parser,\n",
    "    re_graph_builder,\n",
    "    reApproach_finetune,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ababcedd-1449-495b-b6de-cb71c2f28cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Graph Builder configuration:\n",
      "Model name: relation_extraction\n",
      "Graph folder: ./tf_graphs\n",
      "Graph file name: re_graph.pb\n",
      "Build params: {'input_dim': 1149, 'output_dim': 27, 'hidden_layers': [300, 200], 'hidden_act': 'relu', 'hidden_act_l2': True, 'hidden_weights_l2': False, 'batch_norm': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 09:30:48.346809: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 09:30:49.321881: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:\n",
      "2023-08-10 09:30:49.321968: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:\n",
      "2023-08-10 09:30:49.321974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-08-10 09:30:50.111396: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:\n",
      "2023-08-10 09:30:50.111421: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-10 09:30:50.111439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-167-56-117.ec2.internal): /proc/driver/nvidia/version does not exist\n",
      "2023-08-10 09:30:50.111783: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation_extraction graph exported to ./tf_graphs/re_graph.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 20 epochs\n",
      "Epoch 1/20\t0.18s\tLoss: 1.2211732\tACC: 0.29416668\n",
      "Epoch 2/20\t0.02s\tLoss: 0.5527558\tACC: 0.035833336\n",
      "Epoch 3/20\t0.01s\tLoss: 0.24001111\tACC: 0.07083333\n",
      "Epoch 4/20\t0.01s\tLoss: 0.11152315\tACC: 0.18083334\n",
      "Epoch 5/20\t0.01s\tLoss: 0.0957636\tACC: 0.32333332\n",
      "Epoch 6/20\t0.01s\tLoss: 0.08317436\tACC: 0.34833333\n",
      "Epoch 7/20\t0.01s\tLoss: 0.081808046\tACC: 0.33416668\n",
      "Epoch 8/20\t0.01s\tLoss: 0.080394045\tACC: 0.33416668\n",
      "Epoch 9/20\t0.01s\tLoss: 0.069431186\tACC: 0.32083333\n",
      "Epoch 10/20\t0.01s\tLoss: 0.06968339\tACC: 0.27833334\n",
      "Epoch 11/20\t0.01s\tLoss: 0.06706407\tACC: 0.31083333\n",
      "Epoch 12/20\t0.01s\tLoss: 0.067792274\tACC: 0.35166666\n",
      "Epoch 13/20\t0.01s\tLoss: 0.05476083\tACC: 0.3466667\n",
      "Epoch 14/20\t0.01s\tLoss: 0.057616215\tACC: 0.36083335\n",
      "Epoch 15/20\t0.01s\tLoss: 0.055765167\tACC: 0.4475\n",
      "Epoch 16/20\t0.01s\tLoss: 0.05387204\tACC: 0.44416666\n",
      "Epoch 17/20\t0.01s\tLoss: 0.06252569\tACC: 0.40666667\n",
      "Epoch 18/20\t0.01s\tLoss: 0.050994568\tACC: 0.41083333\n",
      "Epoch 19/20\t0.01s\tLoss: 0.040690016\tACC: 0.4575\n",
      "Epoch 20/20\t0.01s\tLoss: 0.04120498\tACC: 0.41916668\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "rel_model = finetune_pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbc7f021-e4f0-4334-ba43-dac12a465bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = rel_model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe225265-5509-4e8a-8037-bd4314182b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_test_df = result.select('rel', 'relations.result').toPandas()\n",
    "pd_test_results = result_test_df.explode(\"result\").fillna(\"O\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "958702d3-eed6-4b29-b5d8-aef10ae897b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           O       0.43      0.92      0.59       274\n",
      "         PIP       0.00      0.00      0.00        88\n",
      "        TeCP       0.00      0.00      0.00        15\n",
      "        TeRP       0.78      0.16      0.26       116\n",
      "        TrAP       0.15      0.03      0.05       108\n",
      "        TrCP       0.00      0.00      0.00        20\n",
      "        TrIP       0.20      0.09      0.13        11\n",
      "       TrNAP       0.00      0.00      0.00         9\n",
      "        TrWP       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.42       650\n",
      "   macro avg       0.17      0.13      0.11       650\n",
      "weighted avg       0.35      0.42      0.30       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = pd_test_results['result']\n",
    "gt = pd_test_results['rel']\n",
    "\n",
    "print(classification_report(gt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326b529-cba5-4f57-a226-870481e33d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42892d5-185a-4e39-a5c0-94f5e029aaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eccc745-b6c7-44ad-9983-ab2b2f8841ae",
   "metadata": {},
   "source": [
    "## Training pipeline(training+ evaluation) with redl models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b713ab90-9da5-4348-ad3f-aab187a0df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'TrIP': '1',\n",
    "'TrAP': '1',\n",
    "'TeCP': '1',\n",
    "'O': '0',\n",
    "'TrNAP': '1',\n",
    "'TrCP': '1',\n",
    "'PIP': '1',\n",
    "'TrWP': '1',\n",
    "'TeRP': '1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a342a578-62e2-43f0-9e8c-ddf205f27740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redl_date_clinical_biobert download started this may take some time.\n",
      "[ | ]redl_date_clinical_biobert download started this may take some time.\n",
      "Approximate size to download 383.1 MB\n",
      "[ — ]Download done! Loading the resource.\n",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 10:42:29.433953: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clinical_re_Model = medical.RelationExtractionDLModel()\\\n",
    "    .pretrained(\"redl_date_clinical_biobert\", \"en\", 'clinical/models')\\\n",
    "    .setInputCols([\"train_ner_chunks\", \"sentences\"])\\\n",
    "    .setOutputCol(\"relations\")\n",
    "\n",
    "clinical_re_Model.getClasses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd96cc5-968c-45c0-abf0-3101245e855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.replace(to_replace=label_map, subset=['rel'])\n",
    "test_data = test_data.replace(to_replace=label_map, subset=['rel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ada8919-f830-472f-81a4-1ae2278957c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_pipeline = nlp.Pipeline(stages=[\n",
    "    documenter,\n",
    "    tokenizer,\n",
    "    words_embedder,\n",
    "    pos_tagger,\n",
    "    dependency_parser,\n",
    "    clinical_re_Model,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38097fec-1131-485e-b70b-5f115123b35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = finetune_pipeline.fit(test_data).transform(test_data)\n",
    "result_test_df = result.select('rel', 'relations.result').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5786d231-8df8-47ba-baea-7188ce18259e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.31      0.35       274\n",
      "           1       0.56      0.65      0.60       376\n",
      "\n",
      "    accuracy                           0.51       650\n",
      "   macro avg       0.48      0.48      0.47       650\n",
      "weighted avg       0.49      0.51      0.49       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd_test_results = result_test_df.explode(\"result\").fillna(\"O\")\n",
    "\n",
    "print(classification_report(pd_test_results[\"rel\"], pd_test_results[\"result\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "170219e2-849c-4d1e-9bcd-f9249eaa078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dependency_conllu_en_3.4.4_3.0_1656845289670\n",
      "embeddings_clinical_en_2.4.0_2.4_1580237286004\n",
      "ner_oncology_en_4.2.2_3.0_1669306355829\n",
      "pos_clinical_en_3.0.0_3.0_1617052315327\n",
      "redl_date_clinical_biobert_en_4.2.4_3.0_1673731277460\n",
      "sbiobert_base_cased_mli_en_2.6.4_2.4_1606225728763\n",
      "sbiobertresolve_snomed_findings_aux_concepts_en_3.1.2_3.0_1645879611162\n",
      "sentence_detector_dl_healthcare_en_3.2.0_3.0_1628678815210\n"
     ]
    }
   ],
   "source": [
    "! ls /root/cache_pretrained/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2231e38-4ab2-48e5-af01-e98c4ed45083",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'johnsnowlabs.medical' has no attribute 'RelationExtractionDLApproach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmedical\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRelationExtractionDLApproach\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'johnsnowlabs.medical' has no attribute 'RelationExtractionDLApproach'"
     ]
    }
   ],
   "source": [
    "medical.RelationExtractionDLApproach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14f9a9e4-6e35-4bec-8f2b-11026c72bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_folder= \"./tf_graphs\"\n",
    "\n",
    "re_graph_builder = medical.TFGraphBuilder()\\\n",
    "    .setModelName(\"relation_extraction\")\\\n",
    "    .setInputCols([\"embeddings\", \"pos_tags\", \"train_ner_chunks\", \"dependencies\"]) \\\n",
    "    .setLabelColumn(\"rel\")\\\n",
    "    .setGraphFolder(graph_folder)\\\n",
    "    .setGraphFile(\"re_graph.pb\")\\\n",
    "    .setHiddenLayers([300, 200])\\\n",
    "    .setHiddenAct(\"relu\")\\\n",
    "    .setHiddenActL2(True)\\\n",
    "    .setHiddenWeightsL2(False)\\\n",
    "    .setBatchNorm(False)\n",
    "\n",
    "reApproach_finetune = medical.RelationExtractionApproach()\\\n",
    "    .setInputCols([\"embeddings\", \"pos_tags\", \"train_ner_chunks\", \"dependencies\"])\\\n",
    "    .setOutputCol(\"relations\")\\\n",
    "    .setLabelColumn(\"rel\")\\\n",
    "    .setEpochsNumber(20)\\\n",
    "    .setBatchSize(200)\\\n",
    "    .setDropout(0.5)\\\n",
    "    .setLearningRate(0.001)\\\n",
    "    .setFixImbalance(True)\\\n",
    "    .setFromEntity(\"firstCharEnt1\", \"lastCharEnt1\", \"label1\")\\\n",
    "    .setToEntity(\"firstCharEnt2\", \"lastCharEnt2\", \"label2\")\\\n",
    "    .setPretrainedModelPath(\"/root/cache_pretrained/redl_date_clinical_biobert_en_4.2.4_3.0_1673731277460\")\\\n",
    "    .setОverrideExistingLabels(False)\n",
    "\n",
    "finetune_pipeline = nlp.Pipeline(stages=[\n",
    "    documenter,\n",
    "    tokenizer,\n",
    "    words_embedder,\n",
    "    pos_tagger,\n",
    "    dependency_parser,\n",
    "    re_graph_builder,\n",
    "    reApproach_finetune,\n",
    "    finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12224fac-6ea5-4014-9ba6-91a49dd49e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Graph Builder configuration:\n",
      "Model name: relation_extraction\n",
      "Graph folder: ./tf_graphs\n",
      "Graph file name: re_graph.pb\n",
      "Build params: {'input_dim': 1149, 'output_dim': 6, 'hidden_layers': [300, 200], 'hidden_act': 'relu', 'hidden_act_l2': True, 'hidden_weights_l2': False, 'batch_norm': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 10:44:33.810121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 10:44:34.724435: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:\n",
      "2023-08-10 10:44:34.724532: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:\n",
      "2023-08-10 10:44:34.724538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-08-10 10:44:35.463520: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:\n",
      "2023-08-10 10:44:35.463544: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-10 10:44:35.463561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-167-57-117.ec2.internal): /proc/driver/nvidia/version does not exist\n",
      "2023-08-10 10:44:35.463912: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation_extraction graph exported to ./tf_graphs/re_graph.pb\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o592.fit.\n: java.lang.ClassCastException: com.johnsnowlabs.nlp.annotators.re.RelationExtractionDLModel cannot be cast to com.johnsnowlabs.nlp.annotators.generic_classifier.GenericClassifierModel\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1(ParamsAndFeaturesReadable.scala:50)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1$adapted(ParamsAndFeaturesReadable.scala:49)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.onRead(ParamsAndFeaturesReadable.scala:49)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$read$1(ParamsAndFeaturesReadable.scala:61)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$read$1$adapted(ParamsAndFeaturesReadable.scala:61)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:38)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:24)\n\tat org.apache.spark.ml.util.MLReadable.load(ReadWrite.scala:355)\n\tat org.apache.spark.ml.util.MLReadable.load$(ReadWrite.scala:355)\n\tat com.johnsnowlabs.nlp.annotators.re.RelationExtractionModel$.load(RelationExtractionModel.scala:549)\n\tat com.johnsnowlabs.nlp.annotators.re.RelationExtractionApproach.train(RelationExtractionApproach.scala:311)\n\tat com.johnsnowlabs.nlp.annotators.re.RelationExtractionApproach.train(RelationExtractionApproach.scala:126)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach._fit(AnnotatorApproach.scala:69)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:75)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rel_model \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/pipeline.py:114\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o592.fit.\n: java.lang.ClassCastException: com.johnsnowlabs.nlp.annotators.re.RelationExtractionDLModel cannot be cast to com.johnsnowlabs.nlp.annotators.generic_classifier.GenericClassifierModel\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1(ParamsAndFeaturesReadable.scala:50)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$onRead$1$adapted(ParamsAndFeaturesReadable.scala:49)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.onRead(ParamsAndFeaturesReadable.scala:49)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$read$1(ParamsAndFeaturesReadable.scala:61)\n\tat com.johnsnowlabs.nlp.ParamsAndFeaturesReadable.$anonfun$read$1$adapted(ParamsAndFeaturesReadable.scala:61)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:38)\n\tat com.johnsnowlabs.nlp.FeaturesReader.load(ParamsAndFeaturesReadable.scala:24)\n\tat org.apache.spark.ml.util.MLReadable.load(ReadWrite.scala:355)\n\tat org.apache.spark.ml.util.MLReadable.load$(ReadWrite.scala:355)\n\tat com.johnsnowlabs.nlp.annotators.re.RelationExtractionModel$.load(RelationExtractionModel.scala:549)\n\tat com.johnsnowlabs.nlp.annotators.re.RelationExtractionApproach.train(RelationExtractionApproach.scala:311)\n\tat com.johnsnowlabs.nlp.annotators.re.RelationExtractionApproach.train(RelationExtractionApproach.scala:126)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach._fit(AnnotatorApproach.scala:69)\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:75)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "rel_model = finetune_pipeline.fit(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f65ce3-c3a6-46a4-ad26-9d4e3a81d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rel_model.transform(test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
